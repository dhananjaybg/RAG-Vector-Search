import json
from pymongo import MongoClient
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import MongoDBAtlasVectorSearch
from langchain_community.document_loaders import DirectoryLoader
from langchain_openai import OpenAI
from langchain.chains import RetrievalQA

from langchain.prompts import PromptTemplate


import gradio as gr
from gradio.themes.base import Base
import params

query = "summarize conversation between alfred and bruce using bullet points?"
query = "what is zlib"
#query = "how much capital was returned to the investors on 2022"
#query = "how much did Bussiness Insurance earn in 2021"

def main():
    mongo_client = create_mongo_client(params.mongodb_conn_string)
    global collection 
    collection = mongo_client.get_database(params.db_name).get_collection(params.collection_name)
    global vs_ 
    vs_ =  CreateVectorStore(collection)
    VectorSearch(query, vs_ ,1)
    GenerateReponse(query, vs_, 3)

    
    with gr.Blocks(theme=Base(), title="Question Answering App using Vector Search + RAG") as demo:
        gr.Markdown(
            """
            # Question Answering App using Atlas Vector Search + RAG Architecture
            """)
        textbox = gr.Textbox(label="Enter your Question:")
        with gr.Row():
            button = gr.Button("Submit", variant="primary")
        with gr.Column():
            output1 = gr.Textbox(lines=1, max_lines=10, label="Output with just Atlas Vector Search (returns text field as is):")
            output2 = gr.Textbox(lines=1, max_lines=10, label="Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + OpenAI LLM:")

        # Call query_data function upon clicking the Submit button
        button.click(call_funcs,textbox, outputs=[output1, output2])

    demo.launch()

def call_funcs(query : str ):
    res1 = VectorSearch(query,vs_, 1)
    res2 = GenerateReponse(query, vs_, 3)
    
    return res1,res2
     

def CreateVectorStore(collection: object ):
    # initialize vector store
    vectorStore = MongoDBAtlasVectorSearch(
        collection, OpenAIEmbeddings(openai_api_key=params.openai_api_key), index_name=params.index_name
    )
    return vectorStore

def VectorSearch(query: str , vectorStore: object , doc_count: int=1):
    print(query)
    display_docs = []
    # perform a similarity search between the embedding of the query and the embeddings of the documents
    print(f"Query Response:") 
    docs = vectorStore.similarity_search_with_score(query, k=doc_count)
    index = 0
    for index, result in enumerate(docs, start = 1):
       #print(f"Document : {index}") 
       print(result[0].page_content)
       display_docs.append(result[0].page_content)
    
    return display_docs[0]

def GenerateReponse(query:str ,vectorStore: object,doc_count: int = 3 ):
    print("\n-------------\nAI Response:\n-------------\n")

    qa_retriever = vectorStore.as_retriever(
        search_type="similarity",
        search_kwargs={"k":doc_count},
    )
    prompt_template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
    {context}
    Question: {question}
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    llm = OpenAI(openai_api_key=params.openai_api_key, temperature=0)
    
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=qa_retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT},
    )
    #x_query = json.dumps({'query':f"{query}"})
    docs = qa(json.dumps({'query':f"{query}"}))
    print(docs["result"])
    return docs["result"]
    #print("\n\n My Reference : ")
    #print(docs["source_documents"])

### Leveraging Atlas Vector Search paired with Langchain's QARetriever
##
### Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model
### If it's not specified (for example like in the code below),
### then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023
##
##llm = OpenAI(openai_api_key=params.openai_api_key, temperature=0)
##
### Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.
### Implements _get_relevant_documents which retrieves documents relevant to a query.
##retriever = vectorStore.as_retriever()
##
### Load "stuff" documents chain. Stuff documents chain takes a list of documents,
### inserts them all into a prompt and passes that prompt to an LLM.
##
##qa = RetrievalQA.from_chain_type(llm, chain_type="stuff", retriever=retriever)
##
### Execute the chain
##
##retriever_output = qa.run(query)
##
### Return Atlas Vector Search output, and output generated using RAG Architecture
##return as_output, retriever_output
def create_mongo_client(uri: str):
    client = MongoClient(uri)
    try:
        client.admin.command('ping')
        print("Pinged your deployment. You successfully connected to MongoDB!")
        return client
    except Exception as e:
        print(e) 


if __name__ == "__main__":
    main()
    